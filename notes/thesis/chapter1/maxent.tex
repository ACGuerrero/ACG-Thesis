\section{El principio de máxima entropía}\label{sec:CH1MaxEnt}



\subsection{El principio de máxima entropía clásico}

Si se nos informa que el valor esperado de tirar un dado de seis caras particular es $3.5$. ¿Cuál es la probabilidad asociada a cada cara del dado? En principio, el problema no puede resolverse, porque la distribución de probabilidad que da como resultado el valor esperado de $3.5$ no es única. Sería igual de válido asumir que el dado está pesado de tal forma que cae la mitad de veces en $2$, y la otra mitad de veces en $5$, que suponer que todos los valores tienen una probabilidad $\frac{1}{6}$. La segunda opción, claro, parecería la más judiciosa. Esta opción es también la que más incertidumbre sobre cada resultado tiene asociada, y la que menos suposiciones sobre los pesos del dado hace. Aún más importante, asumir que todos los valores son igualmente probables equivale a escoger la distribución de probabilidad que maximiza la entropía.

El principio de máxima entropía fue introducido por E. T. Jaynes en 1957. En su artículo, \textit{Information Theory and Statistical Mechanics}: Jaynes afirma que la distribución de probabilidad que maximice la entropía es la mejor estimación que se puede hacer a través de la información disponible, independientemente de si las predicciones coinciden, o no, con los resultados experimentales \cite{JaynesI}. La estimación obtenida a través de la maximización de la entropía es, además, aquella que introduce la menor cantidad de información externa al sistema.

Jaynes relaciona la teoría de información clásica con la mecánica estadística no por la simple coincidencia en la forma de las entropías de Shannon y de Gibbs, sino a través de una reinterpretación de la mecánica estadística como una forma de inferencia estadística. En este contexto, viendo la entropía física como una medida de la incertidumbre asociada a una distribución de probabilidad, una distribución $p$ que no maximice la entropía, es una distribución que introduce información arbitraria no incluída en las hipótesis iniciales. En efecto, el problema de hallar una distribución de probabilidad adecuada es también un problema de contaminación de la información accesible. Esta contaminación proviene de suposiciones arbitrarias, y sin sustento físico, que pueden hacerse sobre el sistema. Utilizar el principio de máxima entropía permite hallar la distribución de probabilidad menos sesgada posible.

Supóngase que los resultados de un proceso corresponden a los valores $x_{i}$ de una variable aleatoria $X$. Sean, además $f_{i}$, funciones sobre $X$, de las que conocemos su valor esperado. La información accesible se puede escribir como el conjunto de restricciones
\begin{equation}\label{eq:JaynesRestrictions}
    \expval{f_{j}(x)}=\sum_{i}p(x_{i})f_{j}(x_{i}).
\end{equation}
Buscamos $p$ tal que maximice la etropía de Shannon (\ref{eq:ShannonEntropy}), sujeta a las restricciones experimentales (\ref{eq:JaynesRestrictions}). Para esto, utilizamos el método de multiplicadores de Lagrange. A las restricciones anteriores debe añadirse 
\begin{equation}\label{eq:NormalizationRestriction}
    \sum_{i}p(x_{i})=1.
\end{equation}
Pedimos que la derivada con respecto a la distribución $p$ de la Lagrangiana
\begin{equation*}
    \mcL=-H(p)+\sum_{j}\lambda_{j}\qty(\sum_{i}p(x_{i})f_{j}(x_{i})-\expval{f_{j}(x)})+\mu\qty(\sum_{i}p(x_{i})-1)
\end{equation*}
se anule. En esta ecuación, $\lambda_{j}$ es el multiplicador de Lagrange que pesa a la $j$-ésima restrición. Derivando e igualando a cero se obtiene que
\begin{gather*}
    \frac{\partial \mcL}{\partial p}=k(1+\log{p(x_{i})})+\sum_{j}\lambda_{j}f_{j}(x_{i})+\mu=0\\
    \Rightarrow p(x_{i})=\exp[-(1+\frac{\mu}{k})-\frac{1}{k}\sum_{j}\lambda_{j}f_{j}(x_{i})].
\end{gather*}
Esta es la distribución que maximiza a la entropía. Respecto a los multiplicadores de Lagrange, nótese que por la restricción (\ref{eq:NormalizationRestriction}) se cumple que
\begin{equation*}
    \frac{1}{e^{-(1+\mu)}}=\sum_{i}\exp[-\frac{1}{k}\sum_{j}\lambda_{j}f_{j}(x_{i})].
\end{equation*}
A partir de esta relación se define a la función de partición,
\begin{equation*}
    Z=\sum_{i}\exp[-\frac{1}{k}\sum_{j}\lambda_{j}f_{j}(x_{i})],
\end{equation*}
que satisface
\begin{equation*}
    \expval{f_{j}(x)}=-\frac{\partial}{\partial \lambda_{j}}\ln{Z}.
\end{equation*}
Considerando lo anterior, el estimado para la distribución de probabilidad que maximiza la entropía, y que es compatible con las restricciones (\ref{eq:JaynesRestrictions}) tiene la forma 
\begin{equation}\label{eq:MaxEntDist}
    p(x_{i})=\frac{1}{Z}\exp[-\frac{1}{k}\sum_{j}\lambda_{j}f_{j}(x_{i})].
\end{equation}

\subsubsection{El estimado de máxima entropía en mecánica estadística}

\acnote{Creo que así queda mejor está sección}

=============================================

El método anterior puede aplicarse a toda una variedad de sistemas y procesos. Considérese, pues, de un conjunto de $N$ partículas idénticas pero distinguibles de energías $\epsilon_{i}$, del cual únicamente conocemos la energía promedio $E$, y que se halla en equilibrio térmico con un entorno que actúa como baño de calor a una temperatura $T$. Nos interesa conocer la distribución de probabilidad de los posibles microestados $\epsilon_{i}$. Según lo discutido previamente, se busca maximizar la entropía,
\begin{equation}\label{eq:GibbsEntropy}
    S_{G}=-k_{B}\sum_{i}p(x_{i})\log{p(x_{i})},
\end{equation}
que es la entropía de Gibbs, con $k_{B}$ la constante de Boltzmann. Continuando con el procedimiento, se halla que la distribución de máxima entropía compatible con la restricción $\expval{\epsilon}=E$ es
\begin{equation}\label{eq:Boltzmann}
    p(x_{i})=\frac{e^{-\frac{\lambda}{k_{B}}\epsilon_{i}}}{Z}.
\end{equation}
Para discernir el significado físico de dicho resultado, se puede sustituir esta distribución en (\ref{eq:Boltzmann}), para encontrar que
\begin{equation}
    S_{G}=\lambda\expval{\epsilon}+k_{B}\ln{Z}.
\end{equation}
Recordando que, en la teoría de la termodinámica se define a la temperatura como el inverso del cambio de la entropía según la energía, se halla el significado físico del multiplicador de Lagrange, pues se tiene que cumplir que
\begin{equation*}
    \lambda=\frac{1}{T}.
\end{equation*}

=============================================

Si aplicáramos el principio de máxima entropía a un sistema físico \ddnote{Ten cuidado aqui, uno siemple aplica esto a sistemas fisicos, la particularidad a resaltar aqui es que se trata de un sistema en equilibrio termico.} como, por ejemplo, el de un conjunto de $N$ partículas idénticas pero distinguibles de energías $\epsilon_{i}$, del cual únicamente conocemos la energía promedio $E$, nos encontraríamos con que
\begin{equation}\label{eq:Boltzman}
    p(x_{i})=\frac{e^{-\lambda\epsilon_{i}}}{Z},
\end{equation}
que reconocemos como la distribución de Boltzmann, en la que $\lambda=\beta=\frac{1}{k_{B}T}$. \ddnote{La idea aqui es que no solo sabemos el promedio de la energia está fijo en la teória), sino que las fluctuaciones alrededor de ese valor son negligibles, debido a que el sistema está en equilibrio térmico y el sistema está además en el límite termódinamico. Bajo esas condiciones es que uno puede hacer la identificación del multiplicador de Lagrange con la temperatura. Dale una leídita al Greiner.}
\subsection{Extensión a la mecánica cuántica}

En su segundo artículo, Jaynes extiende el Principio de máxima entropía a la mecánica cuántica \cite{JaynesII}. Para esto, identificamos al operador de densidad cuántico como la distribución de probabilidad que buscamos hallar, dados algunos resultados experimentales.

Sean, pues, $\hat{A}_{j}$ $m$ operadores hermitianos que operan sobre $\hilbert_{n}$, con $m<n^{2}-1$, de tal forma que formen un conjunto no tomográficamente completo. Dados sus valores esperados,
\begin{equation}\label{eq:QuantumRestrictions}
    \expval{\hat{A}_{j}}=\Tr(\rho \hat{A}_{j}),
\end{equation} 
nos interesa hallar qué operador de densidad es el que mejor descibe a nuestro sistema. Invocando el Principio de máxima entropía, buscamos aquel que maximiza la entropía de von Neumann (\ref{eq:VonNeumannEntropy}). bajo las restricciones (\ref{eq:QuantumRestrictions}) además de
\begin{equation}\label{eq:TraceRestriction}
    \Tr(\rho)=1.
\end{equation}
Una vez más se utiliza el método de los multiplicadores de Lagrange, de manera análoga a como se hizo en el caso clásico. Esta vez, la Lagrangiana es
\begin{equation*}
    \mcL=-S(\rho)+\sum_{j}\lambda_{j}\qty(\expval{\hat{A}_{j}}-\Tr(\rho \hat{A}_{j}))+\mu\qty(\Tr(\rho)-1)
\end{equation*}
y la derivada queda
\begin{equation*}
    \frac{\partial}{\partial \rho}\mcL=
\end{equation*}
\notaAd{No }

El estado de máxima entropía compatible con las observaciones experimentales es entonces
\begin{equation}\label{eq:GeneralMaxEnt}
    \rho=\frac{e^{-\sum_{j} \lambda_{j} \hat{A}_{j}}}{Z}.
\end{equation}
\subsubsection{Caso termodinámico cuántico}
\notaAd{Conocemos $\expval{H}$. El estado de máxima entropía es un estado estacionario. Entonces conmuta con $H$ y comparten eigenbase. ¿Los observables son $H_{kk}$ en la base diagonal? Asumo que esto es los valores esperados respecto a los elementos de la eigenbase. Entonces
\begin{equation*}
    \lambda_{k}=\frac{e^{-\beta H_{kk}}}{Z}
\end{equation*}
¿Y este es el k-ésimo eigenvalor de $\rho$?}
\ddnote{Si, $\lambda_k$ es el $k$-ésimo eigenvalor de $\rho$, pero eso tu lo impones, pues usas la eigenbase de $\rho$. De pronto expliqué esto un poco revuelto. Naturalmente uno puede sustituir $H_{kk}$ por $E_k$ (los eigenvalores de $H$) y la cosa es más clara. La observable es la misma de arriba, la energía $E=\sum_l \lambda_k E_k$, de pronto también te expliqué esto con las patas. La idea entonces, es que una vez tengas los $\lambda_k$, resueltos, uses la notación bra-ket para escribir al final el resultado con los operadores (ahora si independiente de labase).} 
%
\ddnote{Para el argumento de que $[H,\rho]=0$ no olvides usar el argumento de la estacionariedad de $\rho$, dado que no solo se considera un estado de maxima entropía, sino termodinámico como en el caso clásico, uno espera que el estado no cambie respecto al tiempo, por lo tanto conmuta con el Hamiltoniano. Ese es el truco para sacar este estado. También puedes mencionar que siempre que tengas una colección de observables $A_k$ que conmutan con $\rho$ y conoces sus promedios, también puedes calcular el estado MaxEnt. El caso complicado, y es el que mencionarás que derivan en la referencia y que aquí solo mencionamos el resultado, es el caso cuando los $A_k$ no conmutan con $rho$.}

\ddnote{Por fas comienza a usar el comando siguiente:}
\acnote{nota de Adán aquí}