\section{El principio de máxima entropía}\label{sec:CH1MaxEnt}



\subsection{El principio de máxima entropía clásico}

Si se nos informa que el valor esperado de tirar un dado de seis caras particular es $3.5$. ¿Cuál es la probabilidad asociada a cada cara del dado? En principio, el problema no puede resolverse, porque la distribución de probabilidad que da como resultado el valor esperado de $3.5$ no es única. Sería igual de válido asumir que el dado está pesado de tal forma que cae la mitad de veces en $2$, y la otra mitad de veces en $5$, que suponer que todos los valores tienen una probabilidad $\frac{1}{6}$. La segunda opción, claro, parecería la más judiciosa. Esta opción es también la que más incertidumbre sobre cada resultado tiene asociada, y la que menos suposiciones sobre los pesos del dado hace. Aún más importante, asumir que todos los valores son igualmente probables equivale a escoger la distribución de probabilidad que maximiza la entropía.

El principio de máxima entropía fue introducido por E. T. Jaynes en 1957. En su artículo, \textit{Information Theory and Statistical Mechanics}: Jaynes afirma que la distribución de probabilidad que maximice la entropía es la mejor estimación que se puede hacer a través de la información disponible~\cite{JaynesI}. La estimación obtenida a través de la maximización de la entropía es, además, aquella que introduce la menor cantidad de información externa al sistema.

Jaynes relaciona la teoría de información clásica con la mecánica estadística no por la simple coincidencia en la forma de las entropías de Shannon y de Gibbs, sino a través de una reinterpretación de la mecánica estadística como una forma de inferencia estadística. En este contexto, viendo la entropía física como una medida de la incertidumbre asociada a una distribución de probabilidad, una distribución $p$ que no maximice la entropía, es una distribución que introduce información arbitraria no incluida en las hipótesis iniciales. En efecto, el problema de hallar una distribución de probabilidad adecuada es también un problema de contaminación de la información accesible. Esta contaminación proviene de suposiciones arbitrarias, y sin sustento físico, que pueden hacerse sobre el sistema. Utilizar el principio de máxima entropía permite hallar la distribución de probabilidad menos sesgada posible.

\acnote{El siguiente párrafo ha sido iterado, reescritura.}

Sean $x_{j}$ los valores de la variable aleatoria $X$. Sean, además $f_{k}$, funciones sobre $X$, de las que conocemos su valor esperado. La información accesible se puede escribir como el conjunto de restricciones
\begin{equation}\label{eq:JaynesRestrictions}
    \expval{f_{l}(x)}=\sum_{j}p(x_{j})f_{l}(x_{j}).
\end{equation}
\acnote{El siguiente párrafo ha sido iterado, notas}


Buscamos $p$ tal que maximice la entropía de Shannon (\ref{eq:ShannonEntropy}), sujeta a las restricciones experimentales (\ref{eq:JaynesRestrictions}). Para esto, utilizamos el método de multiplicadores de Lagrange. A las restricciones anteriores debe añadirse la condición de normalización,
\begin{equation}\label{eq:NormalizationRestriction}
    \sum_{j}p(x_{j})=1.
\end{equation}
\acnote{Ecuaciones y párrafos iterados: notas y reescritura}

Buscamos el valor extremal respecto a las $p(x_{j})$ de la Lagrangiana
\begin{equation}
    \mcL=-H(p)+\sum_{l}\lambda_{l}\qty(\sum_{j}p(x_{j})f_{l}(x_{j})-\expval{f_{l}(x)})+\mu\qty(\sum_{j}p(x_{j})-1)\nonumber
\end{equation}
En esta ecuación, $\lambda_{l}$ es el multiplicador de Lagrange que pesa a la $j$-ésima restrición. Derivando e igualando a cero se obtiene un sistema de ecuaciones
\begin{equation}
    \frac{\partial \mcL}{\partial p(x_{j})}=k(1+\log{p(x_{j})})+\sum_{l}\lambda_{l}f_{l}(x_{j})+\mu=0,\, j\in\{1,2,3...\},\nonumber
\end{equation}
de donde se obtiene la solución a cada componente
\begin{equation}
    p(x_{j})=\exp[-(1+\frac{\mu}{k})-\frac{1}{k}\sum_{l}\lambda_{l}f_{l}(x_{j})].\nonumber
\end{equation}
Esta es la distribución que maximiza a la entropía. Respecto a los multiplicadores de Lagrange, nótese que por la restricción (\ref{eq:NormalizationRestriction}) se cumple que
\begin{equation}
    \frac{1}{e^{-(1+\mu)}}=\sum_{j}\exp[-\frac{1}{k}\sum_{l}\lambda_{l}f_{l}(x_{j})].\nonumber
\end{equation}
A partir de esta relación se define a la función de partición,
\begin{equation}
    Z=\sum_{j}\exp[-\frac{1}{k}\sum_{l}\lambda_{l}f_{l}(x_{j})],\nonumber
\end{equation}
que satisface
\begin{equation}
    \expval{f_{l}(x)}=-\frac{\partial}{\partial \lambda_{l}}\ln{Z}.\nonumber
\end{equation}
Considerando lo anterior, el estimado para la distribución de probabilidad que maximiza la entropía, y que es compatible con las restricciones (\ref{eq:JaynesRestrictions}) tiene la forma 
\begin{equation}\label{eq:MaxEntDist}
    p(x_{j})=\frac{1}{Z}\exp[-\frac{1}{k}\sum_{l}\lambda_{l}f_{l}(x_{j})].
\end{equation}

\subsubsection{El estimado de máxima entropía en mecánica estadística}

El método anterior puede aplicarse a toda una variedad de sistemas y procesos. Considérese, pues, de un conjunto de $N$ partículas idénticas pero distinguibles de energías $\epsilon_{j}$, del cual únicamente conocemos la energía promedio $E$, y que se halla en equilibrio térmico con un entorno que actúa como baño de calor a una temperatura $T$. Nos interesa conocer la distribución de probabilidad de los posibles microestados $\epsilon_{j}$. Según lo discutido previamente, se busca maximizar la entropía,
\begin{equation}\label{eq:GibbsEntropy}
    S_{G}=-k_{\text{B}}\sum_{j}p(x_{j})\log{p(x_{j})},
\end{equation}
que es la entropía de Gibbs, con $k_{\text{B}}$ la constante de Boltzmann. Continuando con el procedimiento, se halla que la distribución de máxima entropía compatible con la restricción $\expval{\epsilon}=E$ es
\begin{equation}\label{eq:Boltzmann}
    p(x_{j})=\frac{e^{-\frac{\lambda}{k_{\text{B}}}\epsilon_{j}}}{Z}.
\end{equation}
Para discernir el significado físico de dicho resultado, se puede sustituir esta distribución en (\ref{eq:Boltzmann}), para encontrar que
\begin{equation}
    S_{G}=\lambda\expval{\epsilon}+k_{\text{B}}\ln{Z}.
\end{equation}
\acnote{Párrafo iterado: reescritura}

Si tratásemos con un sistema en equilibrio térmico con un reservorio térmico a una temperatura $T$, la física estadística dice que $S$ coincide con la entropía termodinámica y
\begin{equation}
    \lambda=\frac{1}{T}.\nonumber
\end{equation}
con lo que se halla el significado físico del multiplicador de Lagrange. \cite{Greiner}

\subsection{Extensión a la mecánica cuántica}
\acnote{Sección iterada: notas y notación.}

\acnote{Quizá tenga que corregir los límites de las sumas aquí y en la parte clásica}

En su segundo artículo, Jaynes extiende el Principio de máxima entropía a la mecánica cuántica \cite{JaynesII}. Considérese, en un primer momento, el caso en el que se conoce el valor esperado de la energía, $\expval{H}$, para un sistema termodinámico \ddnote*{, es decir, podemos asumir que el estado del sistema es estacionario}{en el que no esperamos que el estado de máxima entropía varíe con el tiempo}. Esta calidad de estado estacionario nos permite suponer que
\begin{equation}
    [\rho,H]=0.\nonumber
\end{equation}
Entonces existe una base en la que tanto $\rho$ como $H$ son diagonales. Sea $\{\ket{e_{k}}\}_{k}$ dicha base, y $E_{k}$ y $\eta_{k}$ los $k$-ésimos elementos diagonales del hamiltoniano $H$ y del operador $\rho$ en ella, respectivamente. La entropía de von Neumann puede escribirse en términos de los eigenvalores de $\rho$ como
\begin{equation}
    S=-\sum_{k}\eta_{k}\log{\eta_{k}}.\nonumber
\end{equation}
Para maximizarla, volvemos a aplicar el método de los multiplicadores de Lagrange. La lagrangiana es
\begin{equation}
    \mcL=\sum_{k}\eta_{k}\log{\eta_{k}}+\lambda_{0}\qty(\sum_{k}\eta_{k}-1)+\lambda_{1}\qty(\sum_{k}\eta_{k}E_{k}-\expval{H}),\nonumber
\end{equation}
en donde la primera restricción es la normalización del estado $\rho$, $\Tr(\rho)=1$, y la segunda corresponde a la información experimental conocida. Nuevamente buscamos el valor extremal respecto a todos los $\eta_{k}$. Derivando e igualando a cero se halla el sistema de ecuaciones \ddnote{haz arreglos a la notación parecidos a los ya discutidos para el caso clásico. Lo que viene abajo es un componente (o en su defecto, una ecuación del sistema de ecuaciones que sale de computar $\frac{\partial \mcL}{\partial \vec \eta}=\vec 0$)} \acnote{queda}
\begin{equation}
    \frac{\partial \mcL}{\partial \eta_{k}}=(1+\log{\eta_{k}})+\lambda_{0}+\lambda_{1}E_{k}=0, \,k\in\{1,2,3...\}\nonumber
\end{equation}
cuyas soluciones son
\begin{equation}
    \eta_{k}=-e^{(\lambda_{0}+1+\lambda_{1}E_{k})}.\nonumber
\end{equation}
Como $\sum_{k}\eta_{k}=1$, podemos definir a la función de partición notando que
\begin{equation}
    e^{1+\lambda_{0}}=\sum_{k}e^{-\lambda_{1}E_{k}}.\nonumber
\end{equation}
Así que la expresión de los eigenvalores del estado que maximiza a la entropía sujeto a la condición $\expval{H}=\sum_{k}\eta_{k}E_{k}$ \ddnote{sujeto a la condición $\langle H \rangle=E$}, $\rho$, en términos de los eigenvalores de $H$, $E_{k}$ es
\begin{equation}
    \eta_{k}=\frac{1}{Z}e^{-\lambda E_{k}}.\nonumber
\end{equation}
Utilizando notación de Dirac, $\rho=\eta_{k}\dyad{e_{k}}$, podemos hallar el resultado independiente de la elección de la base:
\begin{equation}
    \rho=\frac{1}{Z}e^{-\lambda H}.
\end{equation}
Este resultado es fácilmente generalizable al caso en que contemos con un  conjunto $\{A_{j}\}_{j}$ de $N$ observables tales que $[A_{j},\rho]=0$,\ddnote{,} de los que conozcamos sus valores esperados. En general, la expresión del estado de máxima entropía es
\begin{equation}\label{eq:GeneralMaxEnt}
    \rho=\frac{1}{Z}e^{-\sum_{k}\lambda_{k} A_{k}}.
\end{equation}
\ddnote*{Cabe señalar que este resultado también es válido cuando las observables $A_k$ no conmutan entre sí o con $\rho$. Sin embargo la prueba es complicada y está más allá del alcance de esta tesis~\cite{Wichmann1963,FormalJaynes}.}{El caso en que los observables no conmutan con el operador de densidad $\rho$ es un tanto más complicado, y aunque el resultado es el mismo~\cite{FormalJaynes}, en este trabajo no lo demostraremos.}