\section{El principio de máxima entropía}\label{sec:CH1MaxEnt}



\subsection{El principio de máxima entropía clásico}

Si se nos informa que el valor esperado de tirar un dado de seis caras particular es $3.5$. ¿Cuál es la probabilidad asociada a cada cara del dado? En principio, el problema no puede resolverse, porque la distribución de probabilidad que da como resultado el valor esperado de $3.5$ no es única. Sería igual de válido asumir que el dado está pesado de tal forma que cae la mitad de veces en $2$, y la otra mitad de veces en $5$, que suponer que todos los valores tienen una probabilidad $\frac{1}{6}$. La segunda opción, claro, parecería la más judiciosa. Esta opción es también la que más incertidumbre sobre cada resultado tiene asociada, y la que menos suposiciones sobre los pesos del dado hace. Aún más importante, asumir que todos los valores son igualmente probables equivale a escoger la distribución de probabilidad que maximiza la entropía.

El principio de máxima entropía fue introducido por E. T. Jaynes en 1957. En su artículo, \textit{Information Theory and Statistical Mechanics}: Jaynes afirma que la distribución de probabilidad que maximice la entropía es la mejor estimación que se puede hacer a través de la información disponible~\cite{JaynesI}. La estimación obtenida a través de la maximización de la entropía es, además, aquella que introduce la menor cantidad de información externa al sistema.

Jaynes relaciona la teoría de información clásica con la mecánica estadística no por la simple coincidencia en la forma de las entropías de Shannon y de Gibbs, sino a través de una reinterpretación de la mecánica estadística como una forma de inferencia estadística. En este contexto, viendo la entropía física como una medida de la incertidumbre asociada a una distribución de probabilidad, una distribución $p$ que no maximice la entropía, es una distribución que introduce información arbitraria no incluída en las hipótesis iniciales. En efecto, el problema de hallar una distribución de probabilidad adecuada es también un problema de contaminación de la información accesible. Esta contaminación proviene de suposiciones arbitrarias, y sin sustento físico, que pueden hacerse sobre el sistema. Utilizar el principio de máxima entropía permite hallar la distribución de probabilidad menos sesgada posible.

\acnote{El siguiente párrafo ha sido iterado, reescritura.}

Sean $x_{i}$ los valores de la variable aleatoria $X$. \ddnote{no son los resultados de un proceso, sino de los posibles valores de una variables aleatoria.} Sean, además $f_{i}$, funciones sobre $X$, de las que conocemos su valor esperado. La información accesible se puede escribir como el conjunto de restricciones
\begin{equation}\label{eq:JaynesRestrictions}
    \expval{f_{j}(x)}=\sum_{i}p(x_{i})f_{j}(x_{i}).
\end{equation}
\acnote{El siguiente párrafo ha sido iterado, notas}


Buscamos $p$ tal que maximice la entropía de Shannon (\ref{eq:ShannonEntropy}), sujeta a las restricciones experimentales (\ref{eq:JaynesRestrictions}). Para esto, utilizamos el método de multiplicadores de Lagrange. A las restricciones anteriores debe añadirse la condición de normalización, \ddnote{la condición de normalización,}
\begin{equation}\label{eq:NormalizationRestriction}
    \sum_{i}p(x_{i})=1.
\end{equation}
Pedimos que la derivada con respecto a la distribución $p$\ddnote{ten cuidado aquí, buscamos el valor extremal respecto a todas las $p(x_i)$, entonces uno calcula las derivadas respecto a todas las $p(x_i)$, y uno de hecho despeja un sistema de ecuaciones.} \acnote{Me viene bien platicar esto} de la Lagrangiana
\begin{equation}
    \mcL=-H(p)+\sum_{j}\lambda_{j}\qty(\sum_{i}p(x_{i})f_{j}(x_{i})-\expval{f_{j}(x)})+\mu\qty(\sum_{i}p(x_{i})-1)\nonumber
\end{equation}
se anule. En esta ecuación, $\lambda_{j}$ es el multiplicador de Lagrange que pesa a la $j$-ésima restrición. Derivando e igualando a cero se obtiene que
\begin{gather*}
    \frac{\partial \mcL}{\partial p}=k(1+\log{p(x_{i})})+\sum_{j}\lambda_{j}f_{j}(x_{i})+\mu=0\\
    \Rightarrow p(x_{i})=\exp[-(1+\frac{\mu}{k})-\frac{1}{k}\sum_{j}\lambda_{j}f_{j}(x_{i})].
\end{gather*}
Esta es la distribución que maximiza a la entropía. Respecto a los multiplicadores de Lagrange, nótese que por la restricción (\ref{eq:NormalizationRestriction}) se cumple que
\begin{equation*}
    \frac{1}{e^{-(1+\mu)}}=\sum_{i}\exp[-\frac{1}{k}\sum_{j}\lambda_{j}f_{j}(x_{i})].
\end{equation*}
A partir de esta relación se define a la función de partición,
\begin{equation*}
    Z=\sum_{i}\exp[-\frac{1}{k}\sum_{j}\lambda_{j}f_{j}(x_{i})],
\end{equation*}
que satisface
\begin{equation*}
    \expval{f_{j}(x)}=-\frac{\partial}{\partial \lambda_{j}}\ln{Z}.
\end{equation*}
Considerando lo anterior, el estimado para la distribución de probabilidad que maximiza la entropía, y que es compatible con las restricciones (\ref{eq:JaynesRestrictions}) tiene la forma 
\begin{equation}\label{eq:MaxEntDist}
    p(x_{i})=\frac{1}{Z}\exp[-\frac{1}{k}\sum_{j}\lambda_{j}f_{j}(x_{i})].
\end{equation}

\subsubsection{El estimado de máxima entropía en mecánica estadística}

El método anterior puede aplicarse a toda una variedad de sistemas y procesos. Considérese, pues, de un conjunto de $N$ partículas idénticas pero distinguibles de energías $\epsilon_{i}$, del cual únicamente conocemos la energía promedio $E$, y que se halla en equilibrio térmico con un entorno que actúa como baño de calor a una temperatura $T$. Nos interesa conocer la distribución de probabilidad de los posibles microestados $\epsilon_{i}$. Según lo discutido previamente, se busca maximizar la entropía,
\begin{equation}\label{eq:GibbsEntropy}
    S_{G}=-k_{B}\sum_{i}p(x_{i})\log{p(x_{i})},
\end{equation}
que es la entropía de Gibbs, con $k_{B}$ la constante de Boltzmann. Continuando con el procedimiento, se halla que la distribución de máxima entropía compatible con la restricción $\expval{\epsilon}=E$ es
\begin{equation}\label{eq:Boltzmann}
    p(x_{i})=\frac{e^{-\frac{\lambda}{k_{B}}\epsilon_{i}}}{Z}.
\end{equation}
Para discernir el significado físico de dicho resultado, se puede sustituir esta distribución en (\ref{eq:Boltzmann}), para encontrar que
\begin{equation}
    S_{G}=\lambda\expval{\epsilon}+k_{B}\ln{Z}.
\end{equation}
Recordando que, en la teoría de la termodinámica \ddnote{Aquí es mejor decir que en el caso en el que el sistema es está en equilibrio térmico, la termodinámica nos dice blah blah blah} se define a la temperatura como el inverso del cambio de la entropía según la energía \cite{Greiner}, se halla el significado físico del multiplicador de Lagrange, pues se tiene que cumplir que
\begin{equation*}
    \lambda=\frac{1}{T}.
\end{equation*}

\subsection{Extensión a la mecánica cuántica}

En su segundo artículo, Jaynes extiende el Principio de máxima entropía a la mecánica cuántica \cite{JaynesII}. Considérese, en un primer momento, el caso en el que se conoce el valor esperado de la energía, $\expval{H}$, para un sistema termodinámico en el que no esperamos que el estado de máxima entropía varíe con el tiempo. Esta calidad de estado estacionario nos permite afirmar que
\begin{equation*}
    [\rho,H]=0.
\end{equation*}
Gracias a esto podemos afirmar que existe una base en la que tanto $\rho$ como $H$ son diagonales. Sea $\{\ket{e_{k}}\}$ dicha base, y $E_{k}$ y $\eta_{k}$ los $k-$ésimos elementos diagonales del hamiltoniano $H$ y del operador $\rho$ en ella, respectivamente. La entropía de von Neumann puede escribirse en términos de los eigenvalores de $\rho$ como
\begin{equation*}
    S=-\sum_{k}\eta_{k}\log{\eta_{k}}.
\end{equation*}
Para maximizarla, volvemos a aplicar el método de los multiplicadores de Lagrange. La lagrangiana es
\begin{equation*}
    \mcL=\sum_{k}\eta_{k}\log{\eta_{k}}+\lambda_{0}\qty(\sum_{k}\eta_{k}-1)+\lambda_{1}\qty(\sum_{k}\eta_{k}E_{k}-\expval{H}),
\end{equation*}
en donde la primera restricción es la normalización del estado $\rho$, $\Tr(\rho)=1$, y la segunda corresponde a la información experimental conocida. Si se deriva e iguala a cero:
\begin{align*}
    \frac{\partial \mcL}{\partial \eta_{k}}=(1+\log{\eta_{k}})+\lambda_{0}+\lambda_{1}E_{k}=0 & & \Longrightarrow  & &\eta_{k}=-e^{(\lambda_{0}+1+\lambda_{1}E_{k})}.
\end{align*}
Nuevamente, debido a que $\sum_{k}\eta_{k}=1$, podemos definir a la función de partición notando que
\begin{equation*}
    e^{1+\lambda_{0}}=\sum_{k}e^{-\lambda_{1}E_{k}}.
\end{equation*}
Así que la expresión de los eigenvalores del estado que maximiza a la entropía, $\rho$, en términos de los eigenvalores de $H$, $E_{k}$ es
\begin{equation*}
    \eta_{k}=\frac{1}{Z}e^{-\lambda E_{k}}.
\end{equation*}
Utilizando notación de Dirac, $\rho=\eta_{k}\dyad{e_{k}}$, podemos hallar el resultado independiente de la elección de la base:
\begin{equation}
    \rho=\frac{1}{Z}e^{-\lambda H}
\end{equation}
Este resultado es fácilmente generalizable al caso en que contemos con un  conjunto $\{A_{j}\}$ de $N$ observables tales que $[A_{j},\rho]=0$ de los que conozcamos sus valores esperados. En general, la expresión del estado de máxima entropía es
\begin{equation}
    \rho=\frac{1}{Z}e^{-\sum_{k}\lambda_{k} A_{k}}
\end{equation}
El caso en que los observables no conmutan con el operador de densidad $\rho$ es un tanto más complicado, y, aunque el resultado es el mismo \cite{FormalJaynes}, en este trabajo no lo demostraremos.

\acnote{Toda esta sección fue reescrita, después de que me quedara claro qué era lo que tenía que derivar}