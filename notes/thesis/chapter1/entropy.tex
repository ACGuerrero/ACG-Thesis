\section{Entropía}
\subsection{Entropía de Shannon}
A finales de los años cuarenta, Claude Shannon se preguntaba sobre una medida de la \textit{incertidumbre}, o de la \textit{información} asociada a un proceso cuyo resultado estuviera descrito por una variable aleatoria $X$ con distribución de probabilidad $p(x_{i})$.

\acnote{Párrafo iterado: notas}

La cantidad de información provista por el resultado de un experimento depende de la probabilidad asociada a dicho suceso. Por ejemplo, al tirar un dado es mucho menos informativo saber que no cayó un $6$ que saber que cayó un $6$, ya que cada número tiene una probabilidad de $\frac{5}{6}$ de no caer, pero sólo $\frac{1}{6}$ de caer. Con la misma línea de razonamiento, conocer el resultado de un evento que ocurre con probabilidad $p=1$ no transmite ninguna información. Si a cada valor de $X$ se le puede asociar una cantidad de información, entonces debe poder calcularse la cantidad de información promedio: esta es la medida que buscaba Shannon. La forma de esta medida, denotada $H(p)$, vino de las propiedades que el matemático estadounidense afirmó que debía cumplir \cite{Shannon,Wilde}
\begin{enumerate}
    \item $H(p)$ debe ser continua en $p$.
    \item $H(p)$ debe ser una función creciente, monotónica de $n$ cuando $p_{i}=\frac{1}{n}$.
    \item Si $X$ e $Y$ son procesos independientes, $H(p_{X}(x_{i}),p_{Y}(y_{j}))=H(p_{X}(x_{i}))+H(p_{Y}(y_{j}))$.
\end{enumerate}
Además, demostró que
\begin{equation}\label{eq:ShannonEntropy}
    H=-k\sum_{i}p(x_{i})\log{p(x_{i})}.
\end{equation}
Fue a través de discusiones con von Neumann que Shannon descubrió que su medida ya era ampliamente utilizada en física, y que llevaba el nombre de \textit{entropía} \cite{McIrvine}.

Como medida de incertidumbre, la entropía de Shannon (\ref{eq:ShannonEntropy}) es máxima para distribuciones equiprobables. Retomando la idea del dado bien balanceado, como no es posible tener ningún tipo de seguridad sobre el resultado de un tiro, la incertidumbre (la entropía) es máxima.

En teoría de información clásica, la entropía de Shannon se suele estudiar como la cantidad promedio de bits requerida para trasmitir un mensaje. Considérese que se desea transmitir un mensaje encriptado en el que únicamente se utilizan los caracteres A, B, C y D. Si el método de encriptación es tal que resulta que todos los caracteres tienen la misma probabilidad de aparecer, entonces a los caracteres se les pueden asignar los valores 00, 01, 10, y 11 respectivamente. Calculando la entropía de Shannon se halla que, en promedio, se requieren dos bits para transmitir cada caracter del mensaje. 

Supóngase ahora que las probabilidades de que aparezca cada uno de los caracteres son $p(A)=\frac{1}{2}$, $p(B)=\frac{1}{4}$, $p(C)=\frac{1}{8}$, $p(D)=\frac{1}{8}$. En este caso se pueden asignar los valores 1, 10, 110, 111, a cada letra respectivamente. Nótese que ahora solo se requiere un bit para transmitir la letra más común. Pues bien, si se calcula la entropía de Shannon, se encuentra que cada letra requerirá $1.75$ bits para ser transmitida \cite{Chuang}.

\subsection{Entropía de von Neumann}

La entropía de von Neumann, a pesar de haber sido obtenida veinte años antes, puede verse como la extensión cuántica de la entropía clásica de Shannon. Von Neumann introdujo el concepto del operador de densidad de forma paralela e independendiente a L. Landau, y definió la entropía $S$ asociada a un sistema descrito por un operador de densidad $\rho$ como \cite{vonNeumann}
\begin{equation}\label{eq:VonNeumannEntropy}
    S(\rho)=-\Tr(\rho\ln{\rho}).
\end{equation}
\acnote{El siguiente párrafo ha sido iterado, solo notas}

La entropía de von Neumann puede interpretarse de manera similar a la entropía de Shannon. Si se desea transmitir un qubit preparado como $\ket{\psi_{i}}$ con probabilidad $p_{i}$, entonces el operador de densidad que representa al estado enviado es justamente $\rho=\sum p_{i}\dyad{\psi_{i}}$. La cantidad de información recibida, o la incertidumbre sobre el qubit enviado, es justamente $S(\rho)$. Debe hacerse hincapié en el hecho que la entropía de un sistema cuántico debe es fundamentalmente diferente a la de un sistema clásico. El sistema cuántico presenta dos tipos de incertidumbres: la incertidumbre clásica, relacionada a nuestra falta de conocimiento relativa a un sistema, y la incertidumbre cuántica, una propiedad intrínseca a los sistemas ondulatorios, matemáticamente expresada a través del Principio de Incertidumbre de Heisenberg \cite{Wilde}.

\acnote{Lista iterada: notas y reescritura}

De la entropía de von Neumann de un sistema descrito por un operador de densidad $\rho\in\mcS(\hilbert_{n})$ nos interesan las siguientes propiedades \cite{Chuang}:
\begin{enumerate}
    \item La entropía puede escribirse en términos de los eigenvalores de $\rho$, $\eta_{j}$, como $S(\rho)=-\sum_{j}\eta_{j}\ln{\eta_{j}}$. Esto coincide con la entropía de Shannon si se envían los eigenestados de $\rho$ con probabilidades $\eta_{j}$.
    \item La entropía es no negativa, y es nula si y sólo sí $\rho$ es de la forma $\dyad{\psi}$ con $\ket{\psi}\in\hilbert_{n}$.
    \item La entropía es máxima cuando $\rho=\frac{1}{n}\Id_{n}$, y $S(\rho)=n$. Esto es de esperarse, de acuerdo con nuestra discusión previa, el estado máximamente mezclado es aquel del que somo máximamente ignorantes, y por lo mismo debe ser el que tiene la máxima entropía (recordando a la entropía como medida de incertidumbre).
    \item La entropía de un estado producto es igual a la suma de las entropías de cada factor, $S(\rho_{A}\otimes\rho_{B})=S(\rho_{A})+S(\rho_{B})$.
\end{enumerate}