\section{El principio de máxima entropía}
\label{sec:ch2_maxent}



\subsection{El principio de máxima entropía clásico}

Si se nos informa que el valor esperado de tirar un dado de seis caras particular es 3.5 (la variable aleatoria siendo el número de la cara), ¿cuál es la probabilidad asociada a cada cara del dado? En principio, el problema no puede resolverse, porque la distribución de probabilidad que da como resultado el valor esperado de $3.5$ no es única. Sería igual de válido pensar que el dado está pesado de tal forma que cae la mitad de veces en $2$, y la otra mitad de veces en $5$, que suponer que todos los valores tienen una probabilidad $\frac{1}{6}$. La segunda opción, claro, parecería la más judiciosa. Esta opción es también la que más incertidumbre sobre cada resultado tiene asociada, y la que menos suposiciones sobre los pesos del dado hace. Aún más importante, dar por hecho que todos los valores son igualmente probables equivale a escoger la distribución de probabilidad que maximiza la entropía.

\acnote{Párrafo iterado: notas}

El principio de máxima entropía fue introducido por E. T. Jaynes en 1957 \cite{JaynesI}. En su artículo, \textit{Information Theory and Statistical Mechanics}, Jaynes afirma que la distribución de probabilidad que maximice la entropía es la estimación menos sesgada que se puede hacer a través de la información disponible~\cite{JaynesI}. La estimación obtenida a través de la minimización de la entropía es, además, aquella que no introduce suposiciones espurias sobre el sistema.

Jaynes relaciona la teoría de información clásica con la mecánica estadística no por la simple coincidencia en la forma de las entropías de Shannon (\ref{eq:ShannonEntropy}) y de Gibbs (\ref{eq:GibbsEntropy}), sino a través de una reinterpretación de la mecánica estadística como una forma de inferencia estadística. En este contexto, viendo la entropía física como una medida de la incertidumbre asociada a una distribución de probabilidad, una distribución $p$ que no maximice la entropía es una distribución que introduce información arbitraria no incluida en las hipótesis iniciales. En efecto, el problema de hallar una distribución de probabilidad adecuada es también un problema de contaminación de la información accesible. Esta contaminación puede provenir de suposiciones arbitrarias accidentales y sin sustento físico, que pueden hacerse sobre el sistema. Utilizar el principio de máxima entropía permite hallar la distribución de probabilidad menos sesgada posible.

\acnote{El siguiente párrafo ha sido iterado, reescritura.}

Sean $x_{j}$ los valores de la variable aleatoria $X$. Sean, además $f_{k}$, funciones sobre $X$, de las que conocemos su valor esperado. La información accesible se puede escribir como el conjunto de restricciones
\begin{equation}\label{eq:JaynesRestrictions}
    \expval{f_{l}(x)}=\sum_{j}p(x_{j})f_{l}(x_{j}).
\end{equation}
Buscamos $p$ tal que maximice la entropía de Shannon (\ref{eq:ShannonEntropy}), $S_{\text{S}}(p)$, sujeta a las restricciones experimentales (\ref{eq:JaynesRestrictions}). Para esto, utilizamos el método de multiplicadores de Lagrange. A las restricciones anteriores debe añadirse la condición de normalización,
\begin{equation}\label{eq:NormalizationRestriction}
    \sum_{j}p(x_{j})=1.
\end{equation}
Buscamos el valor extremal respecto a las $p(x_{j})$ de la Lagrangiana
\begin{equation}
    \mcL=-S_{\text{S}}(p)+\sum_{l}\lambda_{l}\qty(\sum_{j}p(x_{j})f_{l}(x_{j})-\expval{f_{l}(x)})+\mu\qty(\sum_{j}p(x_{j})-1).\nonumber
\end{equation}
En esta ecuación, $\lambda_{l}$ es el multiplicador de Lagrange que pesa a la $j$-ésima restrición. Derivando e igualando a cero se obtiene un sistema de ecuaciones
\begin{equation}
    \frac{\partial \mcL}{\partial p(x_{j})}=k(1+\log{p(x_{j})})+\sum_{l}\lambda_{l}f_{l}(x_{j})+\mu=0,\, j\in\{1,2,3...\},\nonumber
\end{equation}
de donde se obtiene la solución a cada componente
\begin{equation}
    p(x_{j})=\exp[-(1+\frac{\mu}{k})-\frac{1}{k}\sum_{l}\lambda_{l}f_{l}(x_{j})].\nonumber
\end{equation}
Esta es la distribución que maximiza a la entropía. Respecto a los multiplicadores de Lagrange, nótese que por la restricción (\ref{eq:NormalizationRestriction}) se cumple que
\begin{equation}
    \frac{1}{e^{-(1+\mu)}}=\sum_{j}\exp[-\frac{1}{k}\sum_{l}\lambda_{l}f_{l}(x_{j})].\nonumber
\end{equation}
A partir de esta relación se define a la función de partición,
\begin{equation}
    Z=\sum_{j}\exp[-\frac{1}{k}\sum_{l}\lambda_{l}f_{l}(x_{j})],\nonumber
\end{equation}
que satisface
\begin{equation}
    \expval{f_{l}(x)}=-\frac{\partial}{\partial \lambda_{l}}\ln{Z}.\nonumber
\end{equation}
Considerando lo anterior, el estimado para la distribución de probabilidad que maximiza la entropía, y que es compatible con las restricciones (\ref{eq:JaynesRestrictions}) tiene la forma 
\begin{equation}\label{eq:MaxEntDist}
    p(x_{j})=\frac{1}{Z}\exp[-\frac{1}{k}\sum_{l}\lambda_{l}f_{l}(x_{j})].
\end{equation}

\subsubsection{Principio de máxima entropía en la mecánica estadística}

\acnote{Párrafo iterado: reescritura}

El método anterior puede aplicarse a toda una variedad de sistemas y procesos. Considérese un conjunto de partículas que se halla en equilibrio con un reservorio térmico a temperatura $T$. El sistema tiene niveles energéticos $\epsilon_{j}$, pero únicamente conocemos la energía promedio $E$. Nos interesa conocer la probabilidad asociada a que el sistema se halle en el estado con energía $\epsilon_{j}$. Según lo discutido previamente, se busca maximizar la entropía de Gibbs dada por la ecuación (\ref{eq:GibbsEntropy}). Continuando con el procedimiento, se halla que la distribución de máxima entropía compatible con la restricción $\expval{\epsilon}=E$ es
\begin{equation}\label{eq:Boltzmann}
    p(x_{j})=\frac{e^{-\frac{\lambda}{k_{\text{B}}}\epsilon_{j}}}{Z}.
\end{equation}
Para discernir el significado físico de dicho resultado, se puede sustituir esta distribución en (\ref{eq:Boltzmann}), para encontrar que
\begin{equation}
    S_{G}=\lambda\expval{\epsilon}+k_{\text{B}}\ln{Z}.
\end{equation}

Como tratamos con un sistema en equilibrio térmico con un reservorio a una temperatura $T$, la física estadística establece que $S_{G}$ coincide con la entropía termodinámica y
\begin{equation}
    \lambda=\frac{1}{T},\nonumber
\end{equation}
con lo que se halla el significado físico del multiplicador de Lagrange. \cite{Greiner}

\subsection{Extensión a la mecánica cuántica}
\acnote{Sección iterada: notas y notación.}

\acnote{Quizá tenga que corregir los límites de las sumas aquí y en la parte clásica}

\acnote{Sección iterada: notas}

En su segundo artículo, Jaynes extiende el Principio de máxima entropía a la mecánica cuántica \cite{JaynesII}. Considérese, en un primer momento, el caso en el que se conoce el valor esperado de la energía, $\expval{H}$, para un sistema termodinámico, es decir, podemos considerar que el estado del sistema no varía con el tiempo. Esta calidad de estado estacionario nos permite suponer que
\begin{equation}
    [\rho,H]=0.\nonumber
\end{equation}
Entonces existe una base en la que tanto $\rho$ como $H$ son diagonales. Sea $\{\ket{e_{k}}\}_{k}$ dicha base, y $E_{k}$ y $\eta_{k}$ los $k$-ésimos elementos diagonales del hamiltoniano $H$ y del operador $\rho$ en ella, respectivamente. La entropía de von Neumann puede escribirse en términos de los eigenvalores de $\rho$ como
\begin{equation}
    S_{\text{N}}=-\sum_{k}\eta_{k}\log{\eta_{k}}.\nonumber
\end{equation}
Para maximizarla, volvemos a aplicar el método de los multiplicadores de Lagrange. La lagrangiana es
\begin{equation}
    \mcL=\sum_{k}\eta_{k}\log{\eta_{k}}+\lambda_{0}\qty(\sum_{k}\eta_{k}-1)+\lambda_{1}\qty(\sum_{k}\eta_{k}E_{k}-\expval{H}),\nonumber
\end{equation}
en donde la primera restricción es la normalización del estado $\rho$, $\Tr(\rho)=1$, y la segunda corresponde a la información experimental conocida. Nuevamente buscamos el valor extremal respecto a todos los $\eta_{k}$. Derivando e igualando a cero se halla el sistema de ecuaciones
\begin{equation}
    \frac{\partial \mcL}{\partial \eta_{k}}=(1+\log{\eta_{k}})+\lambda_{0}+\lambda_{1}E_{k}=0, \,k\in\{1,2,3...\}\nonumber
\end{equation}
cuyas soluciones son
\begin{equation}
    \eta_{k}=-e^{(\lambda_{0}+1+\lambda_{1}E_{k})}.\nonumber
\end{equation}
Como $\sum_{k}\eta_{k}=1$, podemos definir a la función de partición notando que
\begin{equation}
    e^{1+\lambda_{0}}=\sum_{k}e^{-\lambda_{1}E_{k}}.\nonumber
\end{equation}
Así que la expresión de los eigenvalores del estado $\rho$ que maximiza a la entropía sujeto a la condición $\expval{H}=\sum_{k}\eta_{k}E_{k}=E$ en términos de los eigenvalores de $H$, $E_{k}$ es
\begin{equation}
    \eta_{k}=\frac{1}{Z}e^{-\lambda E_{k}}.\nonumber
\end{equation}
Utilizando notación de Dirac, $\rho=\sum_{k}\eta_{k}\dyad{e_{k}}$, podemos hallar el resultado independiente de la elección de la base:
\begin{equation}
    \rho=\frac{1}{Z}e^{-\lambda H}.
\end{equation}
Este resultado es generalizable al caso en que contemos con un  conjunto $\{A_{j}\}_{j}$ de $N$ observables tales que $[A_{j},\rho]=0$, de los que conozcamos sus valores esperados. En general, la expresión del estado de máxima entropía es
\begin{equation}\label{eq:GeneralMaxEnt}
    \rho=\frac{1}{Z}e^{-\sum_{k}\lambda_{k} A_{k}}.
\end{equation}
Cabe señalar que este resultado también es válido cuando las observables $A_k$ no conmutan entre sí o con $\rho$. Sin embargo la prueba es complicada y está más allá del alcance de esta tesis~\cite{FormalJaynes,Wichmann1963}.